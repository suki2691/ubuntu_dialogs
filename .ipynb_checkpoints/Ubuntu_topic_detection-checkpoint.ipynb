{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, NMF, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1- Popular Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chat_log_data(path):\n",
    "    '''Function to extract chat data and user names and pickle them'''\n",
    "    \n",
    "    chat_data = []\n",
    "    user_names = []\n",
    "    \n",
    "    #Extracting and storing the chat data and the user names\n",
    "    \n",
    "    for file in os.listdir(path):\n",
    "        filepath = os.path.join(path, file)\n",
    "        with open(filepath,'r') as f:\n",
    "            for line in f:\n",
    "                user_names.append(line.rstrip().split('\\t')[1:3])\n",
    "                chat_data.append(line.rstrip().split('\\t')[-1])\n",
    "     \n",
    "    # Finding the unique user names to remove them from the chat data\n",
    "    \n",
    "    user_names_all = [re.sub('[^A-Za-z .-]+', ' ', j) for i in user_names for j in i]\n",
    "    unique_user_names = list(set(user_names_all))\n",
    "    \n",
    "    # Cleaning the chat data to remove some special characters\n",
    "\n",
    "    chat_data_cleaned = [re.sub('[^A-Za-z .-]+', ' ', i) for i in chat_data]\n",
    "    \n",
    "    #Storing pickles in local directory\n",
    "    with open('comments.pkl', 'wb') as f:\n",
    "        pickle.dump(chat_data_cleaned, f)\n",
    "    \n",
    "    with open('users.pkl', 'wb') as f:\n",
    "        pickle.dump(unique_user_names, f)\n",
    "    \n",
    "    return chat_data_cleaned, unique_user_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def noun_extraction(chat_data_cleaned):\n",
    "    '''Function to tokenize and extract nouns from each sentence'''\n",
    "    NOUNS = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "    \n",
    "    sentences = [nltk.word_tokenize(sent) for sent in chat_data_cleaned]\n",
    "    sentences = [[w for w in sent if nltk.pos_tag([w])[0][1] in NOUNS]\n",
    "                  for sent in sentences]\n",
    "    \n",
    "    with open('sentence_noun.pkl','wb') as f:\n",
    "        pickle.dump(sentences, f)\n",
    "        \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chat_data_cleaned, unique_user_names = chat_log_data('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = noun_extraction(chat_data_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dictionary of the count of the nouns and sorting the dictionary in descending order to get most popular topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = [j for i in sentences for j in i]\n",
    "words_count = defaultdict(int)\n",
    "for word in words:\n",
    "    words_count[word] += 1\n",
    "    \n",
    "words_count_dict = sorted(words_count.items(), key=operator.itemgetter(1),reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 343024),\n",
       " ('t', 152628),\n",
       " ('ubuntu', 133114),\n",
       " ('s', 116190),\n",
       " ('install', 101829),\n",
       " ('use', 91235),\n",
       " ('help', 63944),\n",
       " ('anyone', 61792),\n",
       " ('m', 60585),\n",
       " ('need', 59955),\n",
       " ('don', 52827)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_count_dict[:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the count of words gives only the words that are used more often, it hard to always find the topic with one word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It makes more sense to evaluate bi-grams- find the pairs of words that appear the most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Obtain the bigrams from each sentence\n",
    "bigram_list = []\n",
    "for i in sentences:\n",
    "    bigram_list.append(list(nltk.bigrams(i)))\n",
    "\n",
    "#Flatten the bigram list for easy traversal to create bigram occurence dictionary\n",
    "bigrams = [j for i in bigram_list for j in i] \n",
    "\n",
    "#Counting the occurence of the bigram\n",
    "bigram_count = defaultdict(int)\n",
    "for i in bigrams:\n",
    "    bigram_count[i] += 1\n",
    "bigram_count_dict = sorted(bigram_count.items(), key=operator.itemgetter(1),reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('don', 't'), 52174),\n",
       " (('doesn', 't'), 21141),\n",
       " (('i', 'm'), 18106),\n",
       " (('i', 'need'), 14077),\n",
       " (('sudo', 'apt-get'), 12323),\n",
       " (('apt-get', 'install'), 11996),\n",
       " (('i', 'want'), 11043),\n",
       " (('isn', 't'), 10483),\n",
       " (('i', 'think'), 9318),\n",
       " (('anyone', 'help'), 9292),\n",
       " (('i', 'use'), 9284)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_count_dict[:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2- Topic Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Extracting data from a given file\n",
    "def single_file_chat_log(file_number):\n",
    "    '''Extracting nouns of chat data from the input file'''\n",
    "    \n",
    "    chat_data = []\n",
    "\n",
    "    path = 'data'\n",
    "    filepath = os.path.join(path, str(file_number)+'.tsv')\n",
    "    with open(filepath,'r') as f:\n",
    "        for line in f:\n",
    "            chat_data.append(line.rstrip().split('\\t')[-1])\n",
    "\n",
    "    chat_data_single_file = [re.sub('[^A-Za-z .-]+', ' ', i) for i in chat_data]\n",
    "\n",
    "    # Extracting only the nouns from the sentences\n",
    "    single_file_sentences = noun_extraction(chat_data_single_file)\n",
    "                            \n",
    "    return single_file_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adding the user names to the stopwords matrix to remove occurences in the chat data\n",
    "new_stopwords = new_stopwords = stopwords.words('english') + unique_user_names[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_topic(texts, n_topics,thr=1e-2, **kwargs):\n",
    "    \"\"\"Return keywords of topics \n",
    "    \"\"\"\n",
    "    # Vectorizing the word matrix- using TFIDF Vectorizer\n",
    "    vectorizer = TfidfVectorizer(tokenizer=lambda doc: doc,stop_words=new_stopwords, lowercase=False)\n",
    "    text_vec = vectorizer.fit_transform(texts)\n",
    "    words = np.array(vectorizer.get_feature_names())\n",
    "    \n",
    "    # Applying NMF to obtain topics\n",
    "    topicfinder = NMF(n_topics, **kwargs).fit(text_vec)\n",
    "    topic_dists = topicfinder.components_ \n",
    "    topic_dists /= topic_dists.max(axis = 1).reshape((-1, 1))   \n",
    "    \n",
    "    #finding the keywords for the topics\n",
    "    def _topic_keywords(topic_dist):\n",
    "        keywords_index = np.abs(topic_dist) >= thr\n",
    "        keywords_prefix = np.where(np.sign(topic_dist) > 0, \"\", \"^\")[keywords_index]\n",
    "        keywords = \" | \".join(map(lambda x: \"\".join(x), zip(keywords_prefix, words[keywords_index])))\n",
    "        return keywords\n",
    "    \n",
    "    topic_keywords = map(_topic_keywords, topic_dists)\n",
    "    return \"\\n\".join(\"Topic %i: %s\" % (i, t) for i, t in enumerate(topic_keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change this number to any desired file number\n",
    "file_number = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics of file number: 11\n",
      "Topic 0: changes | config | grandr | panel | permanent | put | right | stops | sure | want\n"
     ]
    }
   ],
   "source": [
    "print('Topics of file number:', file_number)\n",
    "print(find_topic(single_file_chat_log(file_number),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
